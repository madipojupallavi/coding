import sys
import subprocess
import time
import statistics
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import json
import requests
import pandas as pd
from openpyxl import Workbook
from openpyxl.styles import PatternFill, Font
from openpyxl.utils import get_column_letter
from datetime import datetime
from pathlib import Path
import os
import math

# Configuration
URL_AUTH = "https:///token"
URL_PROCESSING = "https:///triggerMailBox"
URL_API = "https:///profiles/{processUid}"
USERNAME = ""
PASSWORD = ""
BODY = "t"
PFX_PATH = ""
PFX_PASSPHRASE = ""
VERIFY_SSL = True

# Test Configuration
TEST_DURATION = 60  # seconds to run the test
MAX_THREADS = 50    # concurrent threads
TOKEN_REFRESH_TIME = 20  # seconds
MAX_STATUS_CHECKS = 10  # Maximum number of times to check for "Completed" status
STATUS_CHECK_INTERVAL = 2  # Seconds between status checks

# Output Configuration
OUTPUT_DIR = Path.cwd() / "performance_results"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_EXCEL = OUTPUT_DIR / f"API_Performance_Report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"

def ensure_requests_pkcs12():
    try:
        import requests_pkcs12
        return requests_pkcs12
    except ImportError:
        subprocess.check_call([sys.executable, "-m", "pip", "install", "requests_pkcs12"])
        import requests_pkcs12
        return requests_pkcs12

class TokenManager:
    def __init__(self, requests_pkcs12):
        self.requests_pkcs12 = requests_pkcs12
        self.token = None
        self.token_timestamp = 0
        self.lock = threading.Lock()
        self.refresh_counter = 0
        self.total_tokens_generated = 0
    
    def get_token(self):
        with self.lock:
            current_time = time.time()
            if self.token is None or (current_time - self.token_timestamp) > TOKEN_REFRESH_TIME:
                self.token = self._fetch_new_token()
                self.token_timestamp = current_time
                self.refresh_counter += 1
                self.total_tokens_generated += 1
                print(f"New token generated (Total refreshes: {self.refresh_counter})")
            return self.token
    
    def _fetch_new_token(self):
        data = {"grant_type": "client_credentials"}
        try:
            start_time = time.time()
            response = self.requests_pkcs12.post(
                URL_AUTH,
                auth=(USERNAME, PASSWORD),
                data=data,
                headers={"Content-Type": "application/x-www-form-urlencoded"},
                pkcs12_filename=PFX_PATH,
                pkcs12_password=PFX_PASSPHRASE,
                verify=VERIFY_SSL,
                timeout=30
            )
            elapsed = time.time() - start_time
            response.raise_for_status()
            
            # Calculate request/response sizes
            request_size = len(str(data)) + len(URL_AUTH) + len(USERNAME) + len(PASSWORD)
            response_size = len(response.text)
            
            return {
                "token": response.json().get("access_token"),
                "response_time": elapsed,
                "request_size_bytes": request_size,
                "response_size_bytes": response_size,
                "success": True
            }
        except Exception as e:
            print(f"Token generation failed: {str(e)}")
            return {
                "token": None,
                "response_time": 0,
                "request_size_bytes": 0,
                "response_size_bytes": 0,
                "success": False,
                "error": str(e)
            }

class TestRunner:
    def __init__(self):
        self.requests_pkcs12 = ensure_requests_pkcs12()
        self.token_manager = TokenManager(self.requests_pkcs12)
        self.results = []
        self.token_stats = []
        self.processing_stats = []
        self.api_stats = []
        self.lock = threading.Lock()
        self.completed_processes = 0
        
    def generate_process_id(self, token):
        headers = {"Authorization": f"Bearer {token}", "Accept": "application/json"}
        try:
            start_time = time.time()
            response = self.requests_pkcs12.get(
                URL_PROCESSING,
                headers=headers,
                pkcs12_filename=PFX_PATH,
                pkcs12_password=PFX_PASSPHRASE,
                verify=VERIFY_SSL,
                timeout=30
            )
            elapsed = time.time() - start_time
            
            # Calculate request/response sizes
            request_size = len(str(headers)) + len(URL_PROCESSING)
            response_size = len(response.text)
            
            response.raise_for_status()
            process_id = response.json().get("processUid")
            
            with self.lock:
                self.processing_stats.append({
                    "process_id": process_id,
                    "response_time": elapsed,
                    "request_size_bytes": request_size,
                    "response_size_bytes": response_size,
                    "success": True
                })
            
            return process_id
        except Exception as e:
            with self.lock:
                self.processing_stats.append({
                    "process_id": None,
                    "response_time": 0,
                    "request_size_bytes": 0,
                    "response_size_bytes": 0,
                    "success": False,
                    "error": str(e)
                })
            print(f"ProcessID generation failed: {str(e)}")
            return None

    def check_api_status(self, token, process_id):
        headers = {"Authorization": f"Bearer {token}", "Accept": "application/json"}
        try:
            start_time = time.time()
            response = self.requests_pkcs12.get(
                URL_API.format(processUid=process_id),
                headers=headers,
                pkcs12_filename=PFX_PATH,
                pkcs12_password=PFX_PASSPHRASE,
                verify=VERIFY_SSL,
                timeout=30
            )
            elapsed = time.time() - start_time
            
            # Calculate request/response sizes
            request_size = len(str(headers)) + len(URL_API.format(processUid=process_id))
            response_size = len(response.text)
            
            # First check if response is 200 OK
            if response.status_code != 200:
                with self.lock:
                    self.api_stats.append({
                        "process_id": process_id,
                        "response_time": elapsed,
                        "request_size_bytes": request_size,
                        "response_size_bytes": response_size,
                        "success": False,
                        "error": f"{response.status_code} {response.reason}",
                        "final_status": "HTTP Error"
                    })
                return {
                    "status": f"{response.status_code} {response.reason}",
                    "response_time": elapsed,
                    "success": False,
                    "final_status": "HTTP Error",
                    "request_size_bytes": request_size,
                    "response_size_bytes": response_size
                }
            
            # If 200 OK, check the JSON response for status
            try:
                response_json = response.json()
                status = response_json.get("status", "Unknown")
                
                # If status is not Completed, poll until it is or max attempts reached
                if status != "Completed":
                    attempts = 1
                    while attempts < MAX_STATUS_CHECKS and status != "Completed":
                        time.sleep(STATUS_CHECK_INTERVAL)
                        check_start = time.time()
                        response = self.requests_pkcs12.get(
                            URL_API.format(processUid=process_id),
                            headers=headers,
                            pkcs12_filename=PFX_PATH,
                            pkcs12_password=PFX_PASSPHRASE,
                            verify=VERIFY_SSL,
                            timeout=30
                        )
                        check_elapsed = time.time() - check_start
                        elapsed += check_elapsed  # Accumulate all check times
                        
                        # Accumulate request/response sizes
                        request_size += len(str(headers)) + len(URL_API.format(processUid=process_id))
                        response_size += len(response.text)
                        
                        if response.status_code != 200:
                            with self.lock:
                                self.api_stats.append({
                                    "process_id": process_id,
                                    "response_time": elapsed,
                                    "request_size_bytes": request_size,
                                    "response_size_bytes": response_size,
                                    "success": False,
                                    "error": f"Status check failed: {response.status_code} {response.reason}",
                                    "final_status": "HTTP Error during polling"
                                })
                            return {
                                "status": f"Status check failed: {response.status_code} {response.reason}",
                                "response_time": elapsed,
                                "success": False,
                                "final_status": "HTTP Error during polling",
                                "request_size_bytes": request_size,
                                "response_size_bytes": response_size
                            }
                        
                        response_json = response.json()
                        status = response_json.get("status", "Unknown")
                        attempts += 1
                
                if status == "Completed":
                    with self.lock:
                        self.api_stats.append({
                            "process_id": process_id,
                            "response_time": elapsed,
                            "request_size_bytes": request_size,
                            "response_size_bytes": response_size,
                            "success": True,
                            "final_status": "Completed"
                        })
                    return {
                        "status": f"200 OK - Completed",
                        "response_time": elapsed,
                        "success": True,
                        "final_status": "Completed",
                        "request_size_bytes": request_size,
                        "response_size_bytes": response_size
                    }
                else:
                    with self.lock:
                        self.api_stats.append({
                            "process_id": process_id,
                            "response_time": elapsed,
                            "request_size_bytes": request_size,
                            "response_size_bytes": response_size,
                            "success": False,
                            "error": f"Final status: {status} (after {attempts} checks)",
                            "final_status": f"Timeout - {status}"
                        })
                    return {
                        "status": f"200 OK - Final status: {status} (after {attempts} checks)",
                        "response_time": elapsed,
                        "success": False,
                        "final_status": f"Timeout - {status}",
                        "request_size_bytes": request_size,
                        "response_size_bytes": response_size
                    }
                    
            except ValueError:  # JSON decode error
                with self.lock:
                    self.api_stats.append({
                        "process_id": process_id,
                        "response_time": elapsed,
                        "request_size_bytes": request_size,
                        "response_size_bytes": response_size,
                        "success": False,
                        "error": "Invalid JSON response",
                        "final_status": "Invalid JSON"
                    })
                return {
                    "status": "200 OK - Invalid JSON response",
                    "response_time": elapsed,
                    "success": False,
                    "final_status": "Invalid JSON",
                    "request_size_bytes": request_size,
                    "response_size_bytes": response_size
                }
                
        except Exception as e:
            with self.lock:
                self.api_stats.append({
                    "process_id": process_id,
                    "response_time": 0,
                    "request_size_bytes": 0,
                    "response_size_bytes": 0,
                    "success": False,
                    "error": str(e),
                    "final_status": f"Exception: {str(e)}"
                })
            return {
                "status": f"Error: {str(e)}",
                "response_time": 0,
                "success": False,
                "final_status": f"Exception: {str(e)}",
                "request_size_bytes": 0,
                "response_size_bytes": 0
            }

    def worker(self):
        try:
            token_result = self.token_manager.get_token()
            if not token_result or not token_result["token"]:
                raise ValueError("Token generation failed")
                
            process_id = self.generate_process_id(token_result["token"])
            if not process_id:
                raise ValueError("Process ID generation failed")
                
            result = self.check_api_status(token_result["token"], process_id)
            
            with self.lock:
                self.completed_processes += 1
                self.results.append({
                    "token_generation_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    "process_id": process_id,
                    "api_status": result["status"],
                    "response_time_sec": result["response_time"],
                    "success": result["success"],
                    "error_type": None if result["success"] else "HTTP",
                    "final_status": result["final_status"],
                    "request_size_bytes": result["request_size_bytes"],
                    "response_size_bytes": result["response_size_bytes"]
                })
                print(f"[{self.completed_processes}] Process {process_id}: {result['status']} ({result['response_time']:.2f}s)")
                
            return result
            
        except Exception as e:
            with self.lock:
                self.completed_processes += 1
                self.results.append({
                    "token_generation_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    "process_id": None,
                    "api_status": f"CRASH: {str(e)}",
                    "response_time_sec": 0,
                    "success": False,
                    "error_type": "SILENT",
                    "final_status": f"Crash: {str(e)}",
                    "request_size_bytes": 0,
                    "response_size_bytes": 0
                })
            print(f"‚ö†Ô∏è Worker failed: {str(e)}")
            return None

    def calculate_metrics(self, data, time_field, size_field=None):
        success_data = [d for d in data if d["success"]]
        success_times = [d[time_field] for d in success_data]
        
        metrics = {
            "count": len(data),
            "success_count": len(success_data),
            "error_count": len(data) - len(success_data),
            "error_rate": (len(data) - len(success_data)) / len(data) * 100 if len(data) > 0 else 0,
            "avg": statistics.mean(success_times) if success_times else 0,
            "min": min(success_times) if success_times else 0,
            "max": max(success_times) if success_times else 0,
            "median": statistics.median(success_times) if success_times else 0,
            "p90": statistics.quantiles(success_times, n=10)[8] if len(success_times) >= 10 else 0,
            "p95": statistics.quantiles(success_times, n=20)[18] if len(success_times) >= 20 else 0,
            "p99": statistics.quantiles(success_times, n=100)[98] if len(success_times) >= 100 else 0,
        }
        
        if size_field:
            total_request_size = sum(d["request_size_bytes"] for d in data)
            total_response_size = sum(d["response_size_bytes"] for d in data)
            metrics.update({
                "total_request_kb": total_request_size / 1024,
                "total_response_kb": total_response_size / 1024,
                "avg_request_kb": (total_request_size / len(data)) / 1024 if len(data) > 0 else 0,
                "avg_response_kb": (total_response_size / len(data)) / 1024 if len(data) > 0 else 0,
            })
        
        return metrics

    def generate_report(self, test_duration):
        try:
            print(f"\nüìä Generating comprehensive report at: {OUTPUT_EXCEL}")
            
            # Calculate metrics for each endpoint
            token_metrics = self.calculate_metrics(self.token_manager.token_stats, "response_time", "request_size_bytes")
            processing_metrics = self.calculate_metrics(self.processing_stats, "response_time", "request_size_bytes")
            api_metrics = self.calculate_metrics(self.api_stats, "response_time", "request_size_bytes")
            
            # Calculate throughput for each endpoint
            token_metrics["throughput"] = token_metrics["count"] / test_duration
            processing_metrics["throughput"] = processing_metrics["count"] / test_duration
            api_metrics["throughput"] = api_metrics["count"] / test_duration
            
            # Calculate KB/sec for each endpoint
            token_metrics["received_kb_sec"] = token_metrics["total_response_kb"] / test_duration
            token_metrics["sent_kb_sec"] = token_metrics["total_request_kb"] / test_duration
            processing_metrics["received_kb_sec"] = processing_metrics["total_response_kb"] / test_duration
            processing_metrics["sent_kb_sec"] = processing_metrics["total_request_kb"] / test_duration
            api_metrics["received_kb_sec"] = api_metrics["total_response_kb"] / test_duration
            api_metrics["sent_kb_sec"] = api_metrics["total_request_kb"] / test_duration
            
            # Create a new Excel workbook
            wb = Workbook()
            
            # Sheet 1: Summary
            ws_summary = wb.active
            ws_summary.title = "Summary"
            
            summary_data = [
                ["Test Duration (seconds)", test_duration],
                ["Total Threads", MAX_THREADS],
                ["Total Tokens Generated", self.token_manager.total_tokens_generated],
                ["Total Process IDs Generated", len(self.processing_stats)],
                ["Total API Calls", len(self.api_stats)],
                ["Total Successful Transactions", len([r for r in self.results if r["success"]])],
                ["Total Failed Transactions", len([r for r in self.results if not r["success"]])],
                ["Overall Error Rate", f"{((len(self.results) - len([r for r in self.results if r['success']])) / len(self.results) * 100 if len(self.results) > 0 else 0):.2f}%"],
                ["Overall Throughput (requests/sec)", f"{len(self.results) / test_duration:.2f}"],
            ]
            
            for row in summary_data:
                ws_summary.append(row)
            
            # Sheet 2: Token Endpoint Metrics
            ws_token = wb.create_sheet("Token Endpoint")
            token_headers = ["Metric", "Value"]
            token_data = [
                ["Total Requests", token_metrics["count"]],
                ["Successful Requests", token_metrics["success_count"]],
                ["Failed Requests", token_metrics["error_count"]],
                ["Error Rate (%)", f"{token_metrics['error_rate']:.2f}"],
                ["Average Latency (s)", f"{token_metrics['avg']:.4f}"],
                ["Minimum Latency (s)", f"{token_metrics['min']:.4f}"],
                ["Maximum Latency (s)", f"{token_metrics['max']:.4f}"],
                ["Median Latency (s)", f"{token_metrics['median']:.4f}"],
                ["90th Percentile (s)", f"{token_metrics['p90']:.4f}"],
                ["95th Percentile (s)", f"{token_metrics['p95']:.4f}"],
                ["99th Percentile (s)", f"{token_metrics['p99']:.4f}"],
                ["Throughput (requests/sec)", f"{token_metrics['throughput']:.2f}"],
                ["Total Sent (KB)", f"{token_metrics['total_request_kb']:.2f}"],
                ["Total Received (KB)", f"{token_metrics['total_response_kb']:.2f}"],
                ["Avg Sent (KB/request)", f"{token_metrics['avg_request_kb']:.2f}"],
                ["Avg Received (KB/request)", f"{token_metrics['avg_response_kb']:.2f}"],
                ["Send Rate (KB/sec)", f"{token_metrics['sent_kb_sec']:.2f}"],
                ["Receive Rate (KB/sec)", f"{token_metrics['received_kb_sec']:.2f}"],
            ]
            
            ws_token.append(token_headers)
            for row in token_data:
                ws_token.append(row)
            
            # Sheet 3: Processing Endpoint Metrics
            ws_processing = wb.create_sheet("Processing Endpoint")
            processing_headers = ["Metric", "Value"]
            processing_data = [
                ["Total Requests", processing_metrics["count"]],
                ["Successful Requests", processing_metrics["success_count"]],
                ["Failed Requests", processing_metrics["error_count"]],
                ["Error Rate (%)", f"{processing_metrics['error_rate']:.2f}"],
                ["Average Latency (s)", f"{processing_metrics['avg']:.4f}"],
                ["Minimum Latency (s)", f"{processing_metrics['min']:.4f}"],
                ["Maximum Latency (s)", f"{processing_metrics['max']:.4f}"],
                ["Median Latency (s)", f"{processing_metrics['median']:.4f}"],
                ["90th Percentile (s)", f"{processing_metrics['p90']:.4f}"],
                ["95th Percentile (s)", f"{processing_metrics['p95']:.4f}"],
                ["99th Percentile (s)", f"{processing_metrics['p99']:.4f}"],
                ["Throughput (requests/sec)", f"{processing_metrics['throughput']:.2f}"],
                ["Total Sent (KB)", f"{processing_metrics['total_request_kb']:.2f}"],
                ["Total Received (KB)", f"{processing_metrics['total_response_kb']:.2f}"],
                ["Avg Sent (KB/request)", f"{processing_metrics['avg_request_kb']:.2f}"],
                ["Avg Received (KB/request)", f"{processing_metrics['avg_response_kb']:.2f}"],
                ["Send Rate (KB/sec)", f"{processing_metrics['sent_kb_sec']:.2f}"],
                ["Receive Rate (KB/sec)", f"{processing_metrics['received_kb_sec']:.2f}"],
            ]
            
            ws_processing.append(processing_headers)
            for row in processing_data:
                ws_processing.append(row)
            
            # Sheet 4: API Endpoint Metrics
            ws_api = wb.create_sheet("API Endpoint")
            api_headers = ["Metric", "Value"]
            api_data = [
                ["Total Requests", api_metrics["count"]],
                ["Successful Requests", api_metrics["success_count"]],
                ["Failed Requests", api_metrics["error_count"]],
                ["Error Rate (%)", f"{api_metrics['error_rate']:.2f}"],
                ["Average Latency (s)", f"{api_metrics['avg']:.4f}"],
                ["Minimum Latency (s)", f"{api_metrics['min']:.4f}"],
                ["Maximum Latency (s)", f"{api_metrics['max']:.4f}"],
                ["Median Latency (s)", f"{api_metrics['median']:.4f}"],
                ["90th Percentile (s)", f"{api_metrics['p90']:.4f}"],
                ["95th Percentile (s)", f"{api_metrics['p95']:.4f}"],
                ["99th Percentile (s)", f"{api_metrics['p99']:.4f}"],
                ["Throughput (requests/sec)", f"{api_metrics['throughput']:.2f}"],
                ["Total Sent (KB)", f"{api_metrics['total_request_kb']:.2f}"],
                ["Total Received (KB)", f"{api_metrics['total_response_kb']:.2f}"],
                ["Avg Sent (KB/request)", f"{api_metrics['avg_request_kb']:.2f}"],
                ["Avg Received (KB/request)", f"{api_metrics['avg_response_kb']:.2f}"],
                ["Send Rate (KB/sec)", f"{api_metrics['sent_kb_sec']:.2f}"],
                ["Receive Rate (KB/sec)", f"{api_metrics['received_kb_sec']:.2f}"],
            ]
            
            ws_api.append(api_headers)
            for row in api_data:
                ws_api.append(row)
            
            # Sheet 5: Detailed Results
            ws_details = wb.create_sheet("Detailed Results")
            details_headers = [
                "Timestamp", "Process ID", "API Status", "Response Time (s)", 
                "Success", "Error Type", "Final Status", "Request Size (bytes)", 
                "Response Size (bytes)"
            ]
            ws_details.append(details_headers)
            
            for result in self.results:
                ws_details.append([
                    result["token_generation_time"],
                    result["process_id"],
                    result["api_status"],
                    result["response_time_sec"],
                    result["success"],
                    result["error_type"],
                    result["final_status"],
                    result["request_size_bytes"],
                    result["response_size_bytes"]
                ])
            
            # Formatting
            header_font = Font(bold=True)
            red_fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
            
            for sheet in wb.worksheets:
                # Format headers
                for cell in sheet[1]:
                    cell.font = header_font
                
                # Auto-adjust column widths
                for col in sheet.columns:
                    max_length = 0
                    column = col[0].column_letter
                    for cell in col:
                        try:
                            if len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    adjusted_width = (max_length + 2) * 1.2
                    sheet.column_dimensions[column].width = adjusted_width
                
                # Highlight failures
                if sheet.title == "Detailed Results":
                    for row in sheet.iter_rows(min_row=2, max_row=len(self.results)+1):
                        if not row[4].value:  # 'success' column
                            for cell in row:
                                cell.fill = red_fill
            
            # Save the workbook
            wb.save(OUTPUT_EXCEL)
            print("‚úÖ Comprehensive report generated successfully")
            
        except PermissionError:
            print(f"‚ùå ERROR: Please close Excel file if open: {OUTPUT_EXCEL}")
            raise
        except Exception as e:
            print(f"‚ùå Failed to generate report: {str(e)}")
            raise

    def run_dynamic_test(self):
        print(f"üöÄ Starting dynamic test for {TEST_DURATION} seconds using {MAX_THREADS} threads...")
        start_time = time.time()
        end_time = start_time + TEST_DURATION
        self.completed_processes = 0  # Reset counter
        
        with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
            futures = []
            while time.time() < end_time:
                futures.append(executor.submit(self.worker))
                # Small sleep to prevent overwhelming the queue
                time.sleep(0.01)
            
            # Wait for remaining tasks to complete
            for future in as_completed(futures):
                future.result()
        
        test_duration = time.time() - start_time
        self.generate_report(test_duration)
        
        print("\n=== DYNAMIC TEST COMPLETE ===")
        print(f"Total duration: {test_duration:.2f}s")
        print(f"Total processes completed: {len(self.results)}")
        print(f"Tokens generated: {self.token_manager.total_tokens_generated}")
        print(f"Process IDs generated: {len(self.processing_stats)}")
        print(f"API calls made: {len(self.api_stats)}")
        print(f"Report saved to: {OUTPUT_EXCEL}")
        
        return self.results

if __name__ == "__main__":
    test = TestRunner()
    results = test.run_dynamic_test()
