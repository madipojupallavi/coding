import sys
import subprocess
import time
import statistics
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import parse_qsl
import json
import requests
requests.packages.urllib3.disable_warnings()

# Configuration
URL_AUTH = "https:///token"
URL_PROCESSING = "https:///triggerMailBox"
URL_API = "https:///profiles/{processUid}"
USERNAME = ""
PASSWORD = ""
BODY = "t"
PFX_PATH = ""
PFX_PASSPHRASE = ""
VERIFY_SSL = True

# Test Configuration
TOTAL_PROCESSES = 50  # Exactly 50 processUids will be generated
MAX_THREADS = 50      # Number of concurrent threads
TOKEN_REFRESH_TIME = 20  # seconds

def ensure_requests_pkcs12():
    try:
        import requests_pkcs12
        return requests_pkcs12
    except ImportError:
        subprocess.check_call([sys.executable, "-m", "pip", "install", "requests_pkcs12"])
        import requests_pkcs12
        return requests_pkcs12

def load_body(body_arg):
    try:
        with open(body_arg, "r", encoding="utf-8") as f:
            raw = f.read().strip()
    except (OSError, IOError):
        raw = body_arg.strip()
    pairs = parse_qsl(raw, keep_blank_values=True)
    return dict(pairs)

class TokenManager:
    def __init__(self, requests_pkcs12):
        self.requests_pkcs12 = requests_pkcs12
        self.token = None
        self.token_timestamp = 0
        self.lock = threading.Lock()
    
    def get_token(self):
        with self.lock:
            current_time = time.time()
            if self.token is None or (current_time - self.token_timestamp) > TOKEN_REFRESH_TIME:
                print("Refreshing token...")
                self.token = self._fetch_new_token()
                self.token_timestamp = current_time
            return self.token
    
    def _fetch_new_token(self):
        data = load_body(BODY)
        try:
            response = self.requests_pkcs12.post(
                URL_AUTH,
                auth=(USERNAME, PASSWORD),
                data=data,
                headers={"Content-Type": "application/x-www-form-urlencoded"},
                pkcs12_filename=PFX_PATH,
                pkcs12_password=PFX_PASSPHRASE,
                verify=VERIFY_SSL,
                timeout=30
            )
            response.raise_for_status()
            return response.json().get("access_token")
        except Exception as e:
            print(f"Token fetch failed: {str(e)}", file=sys.stderr)
            return None

def get_processUid(requests_pkcs12, token):
    headers = {
        "Authorization": f"Bearer {token}",
        "Accept": "application/json"
    }
    try:
        response = requests_pkcs12.get(
            URL_PROCESSING,
            headers=headers,
            pkcs12_filename=PFX_PATH,
            pkcs12_password=PFX_PASSPHRASE,
            verify=VERIFY_SSL,
            timeout=30
        )
        response.raise_for_status()
        processUid = response.json().get("processUid")
        print(f"Generated processUid: {processUid}")
        return processUid
    except Exception as e:
        print(f"Failed to get processUid: {str(e)}", file=sys.stderr)
        return None

def process_request(requests_pkcs12, token_manager, processUid, results):
    token = token_manager.get_token()
    if not token:
        return False
    
    start_time = time.time()
    try:
        headers = {
            "Authorization": f"Bearer {token}",
            "Accept": "application/json"
        }
        response = requests_pkcs12.get(
            URL_API.format(processUid=processUid),
            headers=headers,
            pkcs12_filename=PFX_PATH,
            pkcs12_password=PFX_PASSPHRASE,
            verify=VERIFY_SSL,
            timeout=30
        )
        elapsed = time.time() - start_time
        status = f"{response.status_code} {response.reason}"
        print(f"processUid: {processUid} = {status}")
        
        with results["lock"]:
            results["total_requests"] += 1
            if response.status_code == 200:
                results["successful_requests"] += 1
            else:
                results["failed_requests"] += 1
            results["response_times"].append(elapsed)
        
        return response.status_code == 200
    except Exception as e:
        elapsed = time.time() - start_time
        print(f"processUid: {processUid} = Error: {str(e)}")
        with results["lock"]:
            results["total_requests"] += 1
            results["failed_requests"] += 1
            results["response_times"].append(elapsed)
        return False

def run_test():
    requests_pkcs12 = ensure_requests_pkcs12()
    token_manager = TokenManager(requests_pkcs12)
    
    results = {
        "total_requests": 0,
        "successful_requests": 0,
        "failed_requests": 0,
        "response_times": [],
        "start_time": time.time(),
        "lock": threading.Lock()
    }
    
    # Step 1: Generate all processUids first
    print(f"Generating {TOTAL_PROCESSES} processUids...")
    process_uids = []
    with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
        futures = []
        for _ in range(TOTAL_PROCESSES):
            futures.append(executor.submit(get_processUid, requests_pkcs12, token_manager.get_token()))
        
        for future in as_completed(futures):
            processUid = future.result()
            if processUid:
                process_uids.append(processUid)
    
    if not process_uids:
        print("No processUids were generated", file=sys.stderr)
        return None
    
    # Step 2: Process all generated processUids
    print(f"Processing {len(process_uids)} processUids...")
    with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
        futures = []
        for processUid in process_uids:
            futures.append(executor.submit(
                process_request, 
                requests_pkcs12, 
                token_manager, 
                processUid, 
                results
            ))
        
        for future in as_completed(futures):
            future.result()  # Just wait for completion
    
    results["end_time"] = time.time()
    return results

def analyze_results(results):
    if not results or results["total_requests"] == 0:
        return None

    actual_duration = results["end_time"] - results["start_time"]
    requests_per_second = results["total_requests"] / actual_duration
    error_rate = (results["failed_requests"] / results["total_requests"]) * 100 if results["total_requests"] > 0 else 0

    if results["response_times"]:
        avg_response_time = statistics.mean(results["response_times"])
        min_response_time = min(results["response_times"])
        max_response_time = max(results["response_times"])
        percentile_95 = statistics.quantiles(results["response_times"], n=100)[-1] if len(results["response_times"]) > 1 else results["response_times"][0]
    else:
        avg_response_time = min_response_time = max_response_time = percentile_95 = 0

    return {
        "total_processes": results["total_requests"],
        "successful_processes": results["successful_requests"],
        "failed_processes": results["failed_requests"],
        "error_rate_percent": error_rate,
        "total_duration_seconds": actual_duration,
        "requests_per_second": requests_per_second,
        "avg_response_time_seconds": avg_response_time,
        "min_response_time_seconds": min_response_time,
        "max_response_time_seconds": max_response_time,
        "95th_percentile_seconds": percentile_95
    }

def main():
    test_results = run_test()
    if not test_results:
        print("Test failed to produce results", file=sys.stderr)
        return None
    
    analysis = analyze_results(test_results)
    print("\nTest Results:")
    print(json.dumps(analysis, indent=2))
    return analysis

if __name__ == "__main__":
    main()
