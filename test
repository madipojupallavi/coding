import sys
import subprocess
import time
import statistics
import threading
from concurrent.futures import ThreadPoolExecutor
from urllib.parse import parse_qsl
import json
import requests
requests.packages.urllib3.disable_warnings()

# Configuration
URL_AUTH = "https:///token"
URL_PROCESSING = "https:///triggerMailBox"
URL_API = "https:///profiles/{processUid}"
URL_STATUS = "https:///profiles/{processUid}"  # Same as URL_API if status is in same endpoint
USERNAME = ""
PASSWORD = ""
BODY = "t"
PFX_PATH = ""
PFX_PASSPHRASE = ""
VERIFY_SSL = True

# Performance Test Configuration
TEST_DURATION = 10  # seconds
MAX_THREADS = 15  # concurrent requests
WARMUP_REQUESTS = 3  # warmup requests before testing
TOKEN_REFRESH_TIME = 20  # seconds

def ensure_requests_pkcs12():
    try:
        import requests_pkcs12
        return requests_pkcs12
    except ImportError:
        subprocess.check_call([sys.executable, "-m", "pip", "install", "requests_pkcs12"])
        import requests_pkcs12
        return requests_pkcs12

def load_body(body_arg):
    try:
        with open(body_arg, "r", encoding="utf-8") as f:
            raw = f.read().strip()
    except (OSError, IOError):
        raw = body_arg.strip()
    pairs = parse_qsl(raw, keep_blank_values=True)
    return dict(pairs)

class TokenManager:
    def __init__(self, requests_pkcs12):
        self.requests_pkcs12 = requests_pkcs12
        self.token = None
        self.token_timestamp = 0
        self.lock = threading.Lock()
    
    def get_token(self):
        with self.lock:
            current_time = time.time()
            if self.token is None or (current_time - self.token_timestamp) > TOKEN_REFRESH_TIME:
                self.token = self._fetch_new_token()
                self.token_timestamp = current_time
            return self.token
    
    def _fetch_new_token(self):
        data = load_body(BODY)
        try:
            response = self.requests_pkcs12.post(
                URL_AUTH,
                auth=(USERNAME, PASSWORD),
                data=data,
                headers={"Content-Type": "application/x-www-form-urlencoded"},
                pkcs12_filename=PFX_PATH,
                pkcs12_password=PFX_PASSPHRASE,
                verify=VERIFY_SSL,
                timeout=30
            )
            response.raise_for_status()
            return response.json().get("access_token")
        except Exception as e:
            print(f"Failed to fetch new token: {str(e)}", file=sys.stderr)
            return None

def get_processUid(requests_pkcs12, token):
    headers = {
        "Authorization": f"Bearer {token}",
        "Accept": "application/json"
    }
    try:
        response = requests_pkcs12.get(
            URL_PROCESSING,
            headers=headers,
            pkcs12_filename=PFX_PATH,
            pkcs12_password=PFX_PASSPHRASE,
            verify=VERIFY_SSL,
            timeout=30
        )
        response.raise_for_status()
        return response.json().get("processUid")
    except Exception as e:
        print(f"Failed to get processUid: {str(e)}", file=sys.stderr)
        return None

def check_status(requests_pkcs12, token, processUid):
    headers = {
        "Authorization": f"Bearer {token}",
        "Accept": "application/json"
    }
    try:
        response = requests_pkcs12.get(
            URL_STATUS.format(processUid=processUid),
            headers=headers,
            pkcs12_filename=PFX_PATH,
            pkcs12_password=PFX_PASSPHRASE,
            verify=VERIFY_SSL,
            timeout=30
        )
        response.raise_for_status()
        return response.json().get("status") == "Completed"
    except Exception as e:
        print(f"Failed to check status for {processUid}: {str(e)}", file=sys.stderr)
        return False

def process_request(requests_pkcs12, token_manager, process_uids, results):
    # Get fresh token
    token = token_manager.get_token()
    if not token:
        results["failed_requests"] += 1
        return
    
    # Get new processUid
    processUid = get_processUid(requests_pkcs12, token)
    if not processUid:
        results["failed_requests"] += 1
        return
    
    # Track the new processUid
    with process_uids["lock"]:
        process_uids["ids"].append(processUid)
    
    # Call the API
    start_time = time.time()
    try:
        headers = {
            "Authorization": f"Bearer {token}",
            "Accept": "application/json"
        }
        response = requests_pkcs12.get(
            URL_API.format(processUid=processUid),
            headers=headers,
            pkcs12_filename=PFX_PATH,
            pkcs12_password=PFX_PASSPHRASE,
            verify=VERIFY_SSL,
            timeout=30
        )
        response.raise_for_status()
        elapsed = time.time() - start_time
        
        with results["lock"]:
            results["total_requests"] += 1
            results["successful_requests"] += 1
            results["response_times"].append(elapsed)
    except Exception as e:
        elapsed = time.time() - start_time
        with results["lock"]:
            results["total_requests"] += 1
            results["failed_requests"] += 1
            results["response_times"].append(elapsed)

def monitor_statuses(requests_pkcs12, token_manager, process_uids, stop_event):
    while not stop_event.is_set():
        time.sleep(1)  # Check statuses every second
        
        with process_uids["lock"]:
            current_uids = process_uids["ids"].copy()
        
        token = token_manager.get_token()
        if not token:
            continue
            
        for uid in current_uids:
            if check_status(requests_pkcs12, token, uid):
                with process_uids["lock"]:
                    if uid in process_uids["ids"]:
                        process_uids["ids"].remove(uid)
                print(f"Process {uid} completed successfully")

def run_performance_test(requests_pkcs12):
    test_start_time = time.time()
    token_manager = TokenManager(requests_pkcs12)
    
    # Initialize results and processUid tracker
    results = {
        "total_requests": 0,
        "successful_requests": 0,
        "failed_requests": 0,
        "response_times": [],
        "test_start_time": test_start_time,
        "test_end_time": 0,
        "lock": threading.Lock()
    }
    
    process_uids = {
        "ids": [],
        "lock": threading.Lock()
    }
    
    # Start status monitoring thread
    stop_event = threading.Event()
    monitor_thread = threading.Thread(
        target=monitor_statuses,
        args=(requests_pkcs12, token_manager, process_uids, stop_event)
    )
    monitor_thread.daemon = True
    monitor_thread.start()
    
    # Warmup phase
    print("Running warmup requests...")
    for _ in range(WARMUP_REQUESTS):
        process_request(requests_pkcs12, token_manager, process_uids, results)
    
    print(f"Starting performance test for {TEST_DURATION} seconds with {MAX_THREADS} threads...")
    
    def worker():
        while time.time() - test_start_time < TEST_DURATION:
            process_request(requests_pkcs12, token_manager, process_uids, results)
    
    with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
        futures = [executor.submit(worker) for _ in range(MAX_THREADS)]
        for future in futures:
            future.result()
    
    results["test_end_time"] = time.time()
    stop_event.set()
    monitor_thread.join()
    
    # Wait for remaining processes to complete (with timeout)
    print("Waiting for remaining processes to complete...")
    wait_start = time.time()
    while time.time() - wait_start < 30:  # 30-second timeout
        with process_uids["lock"]:
            if not process_uids["ids"]:
                break
        time.sleep(1)
    
    with process_uids["lock"]:
        if process_uids["ids"]:
            print(f"Warning: {len(process_uids['ids'])} processes did not complete")
    
    return results

def analyze_results(results):
    if not results or results["total_requests"] == 0:
        return None

    actual_duration = results["test_end_time"] - results["test_start_time"]
    requests_per_second = results["total_requests"] / actual_duration
    error_rate = (results["failed_requests"] / results["total_requests"]) * 100

    if results["response_times"]:
        avg_response_time = statistics.mean(results["response_times"])
        min_response_time = min(results["response_times"])
        max_response_time = max(results["response_times"])
        percentile_95 = statistics.quantiles(results["response_times"], n=100)[-1]
    else:
        avg_response_time = min_response_time = max_response_time = percentile_95 = 0

    return {
        "total_test_duration_seconds": actual_duration,
        "total_requests": results["total_requests"],
        "successful_requests": results["successful_requests"],
        "failed_requests": results["failed_requests"],
        "error_rate_percent": error_rate,
        "requests_per_second": requests_per_second,
        "avg_response_time_seconds": avg_response_time,
        "min_response_time_seconds": min_response_time,
        "max_response_time_seconds": max_response_time,
        "95th_percentile_seconds": percentile_95
    }

def main():
    requests_pkcs12 = ensure_requests_pkcs12()
    test_results = run_performance_test(requests_pkcs12)
    if not test_results:
        print("Performance test failed to produce results", file=sys.stderr)
        return None
    analysis = analyze_results(test_results)
    print("\nPerformance Test Results:")
    print(json.dumps(analysis, indent=2))
    return analysis

if __name__ == "__main__":
    main()
